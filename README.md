# Activation Functions Repository

Welcome to the Activation Functions Repository! This repository contains various activation functions commonly used in neural networks. Each activation function is organized into its own subfolder, which includes the function's implementation, equation, graph, and a detailed description.

## Contents

- Sigmoid
- Tanh
- ReLU
- Leaky ReLU
- Softmax

## Folder Structure

Each subfolder contains the following files:

- `function.py`: Python file with the function implementation.
- `equation.png`: Image file displaying the mathematical equation of the activation function.
- `graph.png`: Image file displaying the graph of the activation function.
- `README.md`: Markdown file with a detailed description of the activation function.

## Activation Functions

### Sigmoid

- **Folder**: `Sigmoid`
- **Description**: The sigmoid activation function is a type of logistic function that maps any real-valued number into the range (0, 1). It is often used in binary classification problems.
- **Equation**: Refer to `equation.png` in the `Sigmoid` folder.
- **Graph**: Refer to `graph.png` in the `Sigmoid` folder.

### Tanh

- **Folder**: `Tanh`
- **Description**: The tanh activation function, also known as the hyperbolic tangent function, maps any real-valued number into the range (-1, 1). It is similar to the sigmoid function but outputs values centered around zero.
- **Equation**: Refer to `equation.png` in the `Tanh` folder.
- **Graph**: Refer to `graph.png` in the `Tanh` folder.

### ReLU

- **Folder**: `ReLU`
- **Description**: The ReLU (Rectified Linear Unit) activation function is one of the most popular activation functions in deep learning. It outputs the input directly if it is positive; otherwise, it outputs zero.
- **Equation**: Refer to `equation.png` in the `ReLU` folder.
- **Graph**: Refer to `graph.png` in the `ReLU` folder.

### Leaky ReLU

- **Folder**: `Leaky ReLU`
- **Description**: The Leaky ReLU activation function is an improved version of the ReLU function. It allows a small, non-zero gradient when the input is negative, which helps to keep the information flowing through the network during the training process.
- **Equation**: Refer to `equation.png` in the `Leaky ReLU` folder.
- **Graph**: Refer to `graph.png` in the `Leaky ReLU` folder.

### Softmax

- **Folder**: `Softmax`
- **Description**: The Softmax activation function is used in multi-class classification problems. It converts a vector of values into a probability distribution, where the probabilities of each value are proportional to the exponential of the values.
- **Equation**: Refer to `equation.png` in the `Softmax` folder.
- **Graph**: Refer to `graph.png` in the `Softmax` folder.

## Usage

To use any of these activation functions, navigate to the corresponding folder and refer to the `README.md` file for detailed instructions on implementation and usage.

## Contribution

Contributions are welcome! If you want to add new activation functions or improve the existing ones, feel free to fork this repository, make your changes, and submit a pull request.


---

Thank you for using the Activation Functions Repository! I hope this resource is helpful for your deep learning projects.
